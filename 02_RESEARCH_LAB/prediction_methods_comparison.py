#!/usr/bin/env python3
"""
Ù…Ù‚Ø§Ø±Ù†Ø© Ø´Ø§Ù…Ù„Ø© Ø¨ÙŠÙ† Ø·Ø±Ù‚ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©
Comprehensive Comparison of Prime Number Prediction Methods

Ø£Ø³ØªØ§Ø° Ø¨Ø§Ø³Ù„ ÙŠØ­ÙŠÙ‰ Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡
Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø·Ø±ÙŠÙ‚ØªÙŠÙ†: Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ© ÙˆØ§Ù„Ù…Ø­Ø³Ù†Ø©
"""

import numpy as np
import matplotlib.pyplot as plt
from basil_prime_theory import BasilPrimeTheory, test_prediction_accuracy, generate_primes
from enhanced_prediction_algorithm import EnhancedPrimePrediction
from differential_sphere_model import DifferentialOscillatingSphere
import time
from typing import Dict, List, Tuple
import pandas as pd

class PredictionMethodsComparison:
    """Ù…Ù‚Ø§Ø±Ù†Ø© Ø´Ø§Ù…Ù„Ø© Ø¨ÙŠÙ† Ø·Ø±Ù‚ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
    
    def __init__(self):
        """ØªÙ‡ÙŠØ¦Ø© Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"""
        self.enhanced_predictor = EnhancedPrimePrediction()
        self.results_comparison = {}
        
    def method_1_basic_prediction(self, prime: int) -> Dict:
        """Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ù…Ù† BasilPrimeTheory"""
        
        theory = BasilPrimeTheory(prime)
        start_time = time.time()
        prediction = theory.predict_next_prime(method='basic')
        execution_time = time.time() - start_time
        
        return {
            'method': 'Basic Theory',
            'current_prime': prime,
            'predicted_next': prediction['predicted_next'],
            'confidence': prediction['confidence'],
            'execution_time': execution_time,
            'physical_parameters': {
                'L': theory.inductance,
                'C': theory.capacitance,
                'R': theory.resistance,
                'resonance_error': theory.resonance_error
            }
        }
    
    def method_2_enhanced_prediction(self, prime: int) -> Dict:
        """Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„Ù…Ø­Ø³Ù† Ù…Ù† BasilPrimeTheory"""
        
        theory = BasilPrimeTheory(prime)
        start_time = time.time()
        prediction = theory.predict_next_prime(method='enhanced')
        execution_time = time.time() - start_time
        
        return {
            'method': 'Enhanced Theory',
            'current_prime': prime,
            'predicted_next': prediction['predicted_next'],
            'confidence': prediction['confidence'],
            'execution_time': execution_time,
            'physical_parameters': prediction.get('physical_parameters', {}),
            'attempts': prediction.get('attempts', 0)
        }
    
    def method_3_differential_prediction(self, prime: int) -> Dict:
        """Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø«Ø§Ù„Ø«Ø©: Ø§Ù„ØªÙ†Ø¨Ø¤ Ø§Ù„ØªÙØ§Ø¶Ù„ÙŠ Ù…Ù† DifferentialOscillatingSphere"""

        sphere = DifferentialOscillatingSphere(prime)
        start_time = time.time()
        predicted_next = sphere.predict_next_prime_differential()
        execution_time = time.time() - start_time

        # Ø­Ø³Ø§Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ÙÙŠØ²ÙŠØ§Ø¦ÙŠØ© Ø§Ù„Ù…ØªØ§Ø­Ø©
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø© ÙÙŠ Ø§Ù„ÙƒÙ„Ø§Ø³
            omega_0 = 1 / np.sqrt(sphere.L * sphere.C)
            gamma = sphere.R / (2 * sphere.L)
            quality_factor = omega_0 * sphere.L / sphere.R

            confidence = min(1.0, max(0.1,
                (quality_factor / 10.0 +
                 (1.0 - min(gamma, 1.0)) +
                 (1.0 - sphere.resonance_error)) / 3.0))
        except:
            confidence = 0.5

        return {
            'method': 'Differential Sphere',
            'current_prime': prime,
            'predicted_next': predicted_next,
            'confidence': confidence,
            'execution_time': execution_time,
            'physical_parameters': {
                'L': sphere.L,
                'C': sphere.C,
                'R': sphere.R,
                'resonance_error': sphere.resonance_error
            }
        }
    
    def method_4_advanced_enhanced(self, prime: int) -> Dict:
        """Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„Ø±Ø§Ø¨Ø¹Ø©: Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ø§Ù„Ù…Ø­Ø³Ù†Ø© Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""

        start_time = time.time()
        prediction = self.enhanced_predictor.predict_next_prime_enhanced(prime)
        execution_time = time.time() - start_time

        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©
        gap = prediction.get('estimated_gap', prediction['predicted_next'] - prime)

        return {
            'method': 'Advanced Enhanced',
            'current_prime': prime,
            'predicted_next': prediction['predicted_next'],
            'confidence': prediction['confidence'],
            'execution_time': execution_time,
            'physical_parameters': prediction.get('physical_parameters', {}),
            'gap': gap
        }
    
    def compare_all_methods(self, test_primes: List[int]) -> Dict:
        """Ù…Ù‚Ø§Ø±Ù†Ø© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø·Ø±Ù‚ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©"""
        
        print("ğŸ”¬ Ù…Ù‚Ø§Ø±Ù†Ø© Ø´Ø§Ù…Ù„Ø© Ø¨ÙŠÙ† Ø·Ø±Ù‚ Ø§Ù„ØªÙ†Ø¨Ø¤")
        print("=" * 60)
        
        methods = [
            ('Method 1: Basic', self.method_1_basic_prediction),
            ('Method 2: Enhanced', self.method_2_enhanced_prediction),
            ('Method 3: Differential', self.method_3_differential_prediction),
            ('Method 4: Advanced Enhanced', self.method_4_advanced_enhanced)
        ]
        
        results = {method_name: [] for method_name, _ in methods}
        actual_next_primes = []
        
        # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ø§Ù„ÙØ¹Ù„ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©
        for i, prime in enumerate(test_primes[:-1]):
            actual_next = test_primes[i + 1]
            actual_next_primes.append(actual_next)
        
        # Ø§Ø®ØªØ¨Ø§Ø± ÙƒÙ„ Ø·Ø±ÙŠÙ‚Ø©
        for method_name, method_func in methods:
            print(f"\nğŸ¯ Ø§Ø®ØªØ¨Ø§Ø± {method_name}")
            print("-" * 40)
            
            method_results = []
            correct_predictions = 0
            total_time = 0
            total_confidence = 0
            
            for i, prime in enumerate(test_primes[:-1]):
                try:
                    result = method_func(prime)
                    actual_next = actual_next_primes[i]
                    
                    is_correct = result['predicted_next'] == actual_next
                    gap_error = abs(result['predicted_next'] - actual_next)
                    
                    result.update({
                        'actual_next': actual_next,
                        'is_correct': is_correct,
                        'gap_error': gap_error
                    })
                    
                    method_results.append(result)
                    
                    if is_correct:
                        correct_predictions += 1
                    
                    total_time += result['execution_time']
                    total_confidence += result['confidence']
                    
                    status = "âœ…" if is_correct else "âŒ"
                    print(f"  {status} {prime} â†’ {result['predicted_next']} "
                          f"(ÙØ¹Ù„ÙŠ: {actual_next}, Ø«Ù‚Ø©: {result['confidence']:.2f})")
                
                except Exception as e:
                    print(f"  âŒ Ø®Ø·Ø£ ÙÙŠ {prime}: {str(e)}")
                    continue
            
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
            accuracy = correct_predictions / len(method_results) if method_results else 0
            avg_time = total_time / len(method_results) if method_results else 0
            avg_confidence = total_confidence / len(method_results) if method_results else 0
            avg_gap_error = np.mean([r['gap_error'] for r in method_results]) if method_results else 0
            
            results[method_name] = {
                'predictions': method_results,
                'accuracy': accuracy,
                'correct_predictions': correct_predictions,
                'total_tests': len(method_results),
                'average_time': avg_time,
                'average_confidence': avg_confidence,
                'average_gap_error': avg_gap_error
            }
            
            print(f"ğŸ“Š Ø§Ù„Ù†ØªØ§Ø¦Ø¬: Ø¯Ù‚Ø© {accuracy:.1%}, ÙˆÙ‚Øª {avg_time:.4f}s, Ø«Ù‚Ø© {avg_confidence:.2f}")
        
        return results
    
    def analyze_method_performance(self, comparison_results: Dict) -> Dict:
        """ØªØ­Ù„ÙŠÙ„ Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø·Ø±Ù‚ Ø§Ù„Ù…Ø®ØªÙ„ÙØ©"""
        
        print("\nğŸ“Š ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†")
        print("=" * 50)
        
        analysis = {}
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø·Ø±Ù‚ Ø­Ø³Ø¨ Ø§Ù„Ø¯Ù‚Ø©
        methods_by_accuracy = sorted(
            comparison_results.items(),
            key=lambda x: x[1]['accuracy'],
            reverse=True
        )
        
        print("ğŸ† ØªØ±ØªÙŠØ¨ Ø§Ù„Ø·Ø±Ù‚ Ø­Ø³Ø¨ Ø§Ù„Ø¯Ù‚Ø©:")
        for i, (method, results) in enumerate(methods_by_accuracy, 1):
            print(f"  {i}. {method}: {results['accuracy']:.1%} "
                  f"({results['correct_predictions']}/{results['total_tests']})")
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø·Ø±Ù‚ Ø­Ø³Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø©
        methods_by_speed = sorted(
            comparison_results.items(),
            key=lambda x: x[1]['average_time']
        )
        
        print("\nâš¡ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø·Ø±Ù‚ Ø­Ø³Ø¨ Ø§Ù„Ø³Ø±Ø¹Ø©:")
        for i, (method, results) in enumerate(methods_by_speed, 1):
            print(f"  {i}. {method}: {results['average_time']:.4f}s")
        
        # ØªØ±ØªÙŠØ¨ Ø§Ù„Ø·Ø±Ù‚ Ø­Ø³Ø¨ Ø§Ù„Ø«Ù‚Ø©
        methods_by_confidence = sorted(
            comparison_results.items(),
            key=lambda x: x[1]['average_confidence'],
            reverse=True
        )
        
        print("\nğŸ¯ ØªØ±ØªÙŠØ¨ Ø§Ù„Ø·Ø±Ù‚ Ø­Ø³Ø¨ Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø«Ù‚Ø©:")
        for i, (method, results) in enumerate(methods_by_confidence, 1):
            print(f"  {i}. {method}: {results['average_confidence']:.2f}")
        
        # ØªØ­Ù„ÙŠÙ„ Ø®Ø·Ø£ Ø§Ù„ÙØ¬ÙˆØ©
        print("\nğŸ“ Ù…ØªÙˆØ³Ø· Ø®Ø·Ø£ Ø§Ù„ÙØ¬ÙˆØ©:")
        for method, results in comparison_results.items():
            print(f"  {method}: {results['average_gap_error']:.2f}")
        
        # ØªØ­Ø¯ÙŠØ¯ Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ø´Ø§Ù…Ù„Ø©
        best_method = self._calculate_overall_best_method(comparison_results)
        
        analysis = {
            'best_accuracy': methods_by_accuracy[0],
            'fastest_method': methods_by_speed[0],
            'highest_confidence': methods_by_confidence[0],
            'overall_best': best_method,
            'detailed_comparison': comparison_results
        }
        
        print(f"\nğŸ† Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ø´Ø§Ù…Ù„Ø©: {best_method[0]}")
        
        return analysis
    
    def _calculate_overall_best_method(self, results: Dict) -> Tuple[str, Dict]:
        """Ø­Ø³Ø§Ø¨ Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ù…Ø¹Ø§ÙŠÙŠØ± Ù…ØªØ¹Ø¯Ø¯Ø©"""
        
        scores = {}
        
        for method, data in results.items():
            # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù†Ù‚Ø§Ø· (ÙƒÙ„Ù…Ø§ Ø²Ø§Ø¯Øª ÙƒÙ„Ù…Ø§ ÙƒØ§Ù† Ø£ÙØ¶Ù„)
            accuracy_score = data['accuracy'] * 100  # 0-100
            speed_score = max(0, 100 - data['average_time'] * 1000)  # ÙƒÙ„Ù…Ø§ Ù‚Ù„ Ø§Ù„ÙˆÙ‚Øª Ø²Ø§Ø¯Øª Ø§Ù„Ù†Ù‚Ø§Ø·
            confidence_score = data['average_confidence'] * 100  # 0-100
            gap_error_score = max(0, 100 - data['average_gap_error'] * 10)  # ÙƒÙ„Ù…Ø§ Ù‚Ù„ Ø§Ù„Ø®Ø·Ø£ Ø²Ø§Ø¯Øª Ø§Ù„Ù†Ù‚Ø§Ø·
            
            # Ø§Ù„ÙˆØ²Ù† Ø§Ù„Ù†Ø³Ø¨ÙŠ Ù„Ù„Ù…Ø¹Ø§ÙŠÙŠØ±
            total_score = (
                accuracy_score * 0.4 +      # Ø§Ù„Ø¯Ù‚Ø© Ø£Ù‡Ù… Ù…Ø¹ÙŠØ§Ø± (40%)
                confidence_score * 0.3 +    # Ø§Ù„Ø«Ù‚Ø© (30%)
                gap_error_score * 0.2 +     # Ø®Ø·Ø£ Ø§Ù„ÙØ¬ÙˆØ© (20%)
                speed_score * 0.1           # Ø§Ù„Ø³Ø±Ø¹Ø© (10%)
            )
            
            scores[method] = total_score
        
        best_method = max(scores.items(), key=lambda x: x[1])
        return best_method[0], results[best_method[0]]
    
    def plot_comparison_results(self, comparison_results: Dict, analysis: Dict):
        """Ø±Ø³Ù… Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"""
        
        methods = list(comparison_results.keys())
        accuracies = [comparison_results[m]['accuracy'] * 100 for m in methods]
        times = [comparison_results[m]['average_time'] * 1000 for m in methods]  # milliseconds
        confidences = [comparison_results[m]['average_confidence'] * 100 for m in methods]
        gap_errors = [comparison_results[m]['average_gap_error'] for m in methods]
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # Ø±Ø³Ù… Ø§Ù„Ø¯Ù‚Ø©
        bars1 = axes[0, 0].bar(methods, accuracies, color=['blue', 'green', 'red', 'orange'], alpha=0.7)
        axes[0, 0].set_title('Prediction Accuracy Comparison', fontsize=14, fontweight='bold')
        axes[0, 0].set_ylabel('Accuracy (%)')
        axes[0, 0].set_ylim(0, 105)
        axes[0, 0].grid(True, alpha=0.3)
        
        # Ø¥Ø¶Ø§ÙØ© Ù‚ÙŠÙ… Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©
        for bar, acc in zip(bars1, accuracies):
            axes[0, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                           f'{acc:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        # Ø±Ø³Ù… Ø§Ù„ÙˆÙ‚Øª
        bars2 = axes[0, 1].bar(methods, times, color=['blue', 'green', 'red', 'orange'], alpha=0.7)
        axes[0, 1].set_title('Execution Time Comparison', fontsize=14, fontweight='bold')
        axes[0, 1].set_ylabel('Time (ms)')
        axes[0, 1].grid(True, alpha=0.3)
        
        for bar, time_val in zip(bars2, times):
            axes[0, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                           f'{time_val:.2f}ms', ha='center', va='bottom', fontweight='bold')
        
        # Ø±Ø³Ù… Ø§Ù„Ø«Ù‚Ø©
        bars3 = axes[1, 0].bar(methods, confidences, color=['blue', 'green', 'red', 'orange'], alpha=0.7)
        axes[1, 0].set_title('Confidence Level Comparison', fontsize=14, fontweight='bold')
        axes[1, 0].set_ylabel('Confidence (%)')
        axes[1, 0].set_ylim(0, 105)
        axes[1, 0].grid(True, alpha=0.3)
        
        for bar, conf in zip(bars3, confidences):
            axes[1, 0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,
                           f'{conf:.1f}%', ha='center', va='bottom', fontweight='bold')
        
        # Ø±Ø³Ù… Ø®Ø·Ø£ Ø§Ù„ÙØ¬ÙˆØ©
        bars4 = axes[1, 1].bar(methods, gap_errors, color=['blue', 'green', 'red', 'orange'], alpha=0.7)
        axes[1, 1].set_title('Gap Error Comparison', fontsize=14, fontweight='bold')
        axes[1, 1].set_ylabel('Average Gap Error')
        axes[1, 1].grid(True, alpha=0.3)
        
        for bar, error in zip(bars4, gap_errors):
            axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,
                           f'{error:.2f}', ha='center', va='bottom', fontweight='bold')
        
        # ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªØ®Ø·ÙŠØ·
        for ax in axes.flat:
            ax.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('prediction_methods_comparison.png', dpi=300, bbox_inches='tight')
        print("âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„Ø±Ø³Ù…: prediction_methods_comparison.png")
        
        return fig

def main():
    """Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©"""
    
    print("ğŸš€ Ù…Ù‚Ø§Ø±Ù†Ø© Ø´Ø§Ù…Ù„Ø© Ø¨ÙŠÙ† Ø·Ø±Ù‚ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ©")
    print("=" * 70)
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø£Ø¯Ø§Ø© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
    comparator = PredictionMethodsComparison()
    
    # Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø£Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø£ÙˆÙ„ÙŠØ© Ù„Ù„Ø§Ø®ØªØ¨Ø§Ø±
    test_primes = [5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79]
    
    print(f"ğŸ“Š Ø§Ø®ØªØ¨Ø§Ø± Ø¹Ù„Ù‰ {len(test_primes)-1} ØªÙ†Ø¨Ø¤")
    print(f"ğŸ¯ Ø§Ù„Ù†Ø·Ø§Ù‚: {test_primes[0]} Ø¥Ù„Ù‰ {test_primes[-2]} â†’ {test_primes[-1]}")
    
    # ØªØ´ØºÙŠÙ„ Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
    comparison_results = comparator.compare_all_methods(test_primes)
    
    # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    analysis = comparator.analyze_method_performance(comparison_results)
    
    # Ø±Ø³Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬
    fig = comparator.plot_comparison_results(comparison_results, analysis)
    
    # Ø·Ø¨Ø§Ø¹Ø© Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©
    print("\n" + "="*70)
    print("ğŸ† Ø§Ù„Ø®Ù„Ø§ØµØ© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ©:")
    print("="*70)
    
    best_method = analysis['overall_best']
    print(f"ğŸ¥‡ Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ø´Ø§Ù…Ù„Ø©: {best_method[0]}")
    print(f"   ğŸ“Š Ø§Ù„Ø¯Ù‚Ø©: {best_method[1]['accuracy']:.1%}")
    print(f"   âš¡ Ø§Ù„ÙˆÙ‚Øª: {best_method[1]['average_time']:.4f}s")
    print(f"   ğŸ¯ Ø§Ù„Ø«Ù‚Ø©: {best_method[1]['average_confidence']:.2f}")
    print(f"   ğŸ“ Ø®Ø·Ø£ Ø§Ù„ÙØ¬ÙˆØ©: {best_method[1]['average_gap_error']:.2f}")
    
    print(f"\nğŸ¥ˆ Ø£Ø¹Ù„Ù‰ Ø¯Ù‚Ø©: {analysis['best_accuracy'][0]} ({analysis['best_accuracy'][1]['accuracy']:.1%})")
    print(f"ğŸ¥‰ Ø£Ø³Ø±Ø¹ Ø·Ø±ÙŠÙ‚Ø©: {analysis['fastest_method'][0]} ({analysis['fastest_method'][1]['average_time']:.4f}s)")
    
    return comparison_results, analysis

if __name__ == "__main__":
    results, analysis = main()
